{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 20:40:23,643 - INFO - Data loaded successfully\n",
      "2025-06-04 20:40:23,717 - INFO - Feature engineering completed\n",
      "2025-06-04 20:40:23,721 - INFO - NaN counts in train_data: temperature                     1001\n",
      "irradiance                      1411\n",
      "humidity                         127\n",
      "panel_age                       1011\n",
      "maintenance_count               1027\n",
      "soiling_ratio                   1010\n",
      "voltage                          993\n",
      "current                          977\n",
      "module_temperature               978\n",
      "cloud_coverage                  1010\n",
      "wind_speed                       119\n",
      "pressure                         135\n",
      "power_output                    1925\n",
      "temp_adjusted_irradiance        1411\n",
      "soiling_impact                  2363\n",
      "age_maintenance_ratio           1011\n",
      "log_irradiance                     0\n",
      "temp_diff                       1947\n",
      "irradiance_per_age              1411\n",
      "wind_cooling                     119\n",
      "humidity_pressure_ratio          127\n",
      "cloud_irradiance_interaction    2333\n",
      "dtype: int64\n",
      "2025-06-04 20:40:23,727 - INFO - Target variable dtype: float64, NaN count: 0\n",
      "[I 2025-06-04 20:40:24,857] A new study created in RDB with name: solar_panel_efficiency\n",
      "[W 2025-06-04 20:40:26,505] Trial 2 failed with parameters: {'xgb_n_estimators': 2598, 'xgb_lr': 0.2201584518264578, 'xgb_max_depth': 9, 'xgb_subsample': 0.8143208491610804, 'xgb_colsample': 0.9911189400191849, 'xgb_min_child_weight': 7.724188629347093, 'xgb_gamma': 2.5942356369119906, 'lgbm_n_estimators': 2246, 'lgbm_lr': 0.13048773331517632, 'lgbm_max_depth': 9, 'lgbm_subsample': 0.773616623708568, 'lgbm_colsample': 0.9549021053589732, 'lgbm_num_leaves': 54, 'lgbm_min_child_samples': 22, 'rf_n_estimators': 291, 'rf_max_depth': 4, 'rf_min_samples_split': 7, 'rf_min_samples_leaf': 1} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,511] Trial 7 failed with parameters: {'xgb_n_estimators': 2226, 'xgb_lr': 0.18308289688678187, 'xgb_max_depth': 11, 'xgb_subsample': 0.6909383187645095, 'xgb_colsample': 0.5483222741580258, 'xgb_min_child_weight': 2.45008611353582, 'xgb_gamma': 0.3545704231283553, 'lgbm_n_estimators': 2930, 'lgbm_lr': 0.005078622297892986, 'lgbm_max_depth': 4, 'lgbm_subsample': 0.9445220101361685, 'lgbm_colsample': 0.7006063003244014, 'lgbm_num_leaves': 80, 'lgbm_min_child_samples': 33, 'rf_n_estimators': 288, 'rf_max_depth': 4, 'rf_min_samples_split': 5, 'rf_min_samples_leaf': 1} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,511] Trial 2 failed with value None.\n",
      "[W 2025-06-04 20:40:26,515] Trial 7 failed with value None.\n",
      "[W 2025-06-04 20:40:26,801] Trial 1 failed with parameters: {'xgb_n_estimators': 561, 'xgb_lr': 0.21058856135195436, 'xgb_max_depth': 10, 'xgb_subsample': 0.7598886321108929, 'xgb_colsample': 0.8523332649481883, 'xgb_min_child_weight': 1.8906708338700917, 'xgb_gamma': 4.057452315686374, 'lgbm_n_estimators': 1328, 'lgbm_lr': 0.21236135316403756, 'lgbm_max_depth': 6, 'lgbm_subsample': 0.8350812043091417, 'lgbm_colsample': 0.9822555693270385, 'lgbm_num_leaves': 42, 'lgbm_min_child_samples': 49, 'rf_n_estimators': 790, 'rf_max_depth': 6, 'rf_min_samples_split': 18, 'rf_min_samples_leaf': 7} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,804] Trial 10 failed with parameters: {'xgb_n_estimators': 1810, 'xgb_lr': 0.07966130446776101, 'xgb_max_depth': 10, 'xgb_subsample': 0.9371400997016144, 'xgb_colsample': 0.8781348346066558, 'xgb_min_child_weight': 7.586912350830782, 'xgb_gamma': 2.360882379088816, 'lgbm_n_estimators': 2269, 'lgbm_lr': 0.09406514818028969, 'lgbm_max_depth': 3, 'lgbm_subsample': 0.9556449007011315, 'lgbm_colsample': 0.5865638122095589, 'lgbm_num_leaves': 105, 'lgbm_min_child_samples': 45, 'rf_n_estimators': 745, 'rf_max_depth': 5, 'rf_min_samples_split': 11, 'rf_min_samples_leaf': 10} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,807] Trial 1 failed with value None.\n",
      "[W 2025-06-04 20:40:26,807] Trial 9 failed with parameters: {'xgb_n_estimators': 1793, 'xgb_lr': 0.03559298882162514, 'xgb_max_depth': 10, 'xgb_subsample': 0.8432894696626687, 'xgb_colsample': 0.9311762861607923, 'xgb_min_child_weight': 9.60890803007619, 'xgb_gamma': 2.289061288314624, 'lgbm_n_estimators': 665, 'lgbm_lr': 0.020231409069329933, 'lgbm_max_depth': 7, 'lgbm_subsample': 0.5797497308859543, 'lgbm_colsample': 0.8986010809765853, 'lgbm_num_leaves': 31, 'lgbm_min_child_samples': 11, 'rf_n_estimators': 240, 'rf_max_depth': 10, 'rf_min_samples_split': 10, 'rf_min_samples_leaf': 1} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,811] Trial 10 failed with value None.\n",
      "[W 2025-06-04 20:40:26,816] Trial 9 failed with value None.\n",
      "[W 2025-06-04 20:40:26,909] Trial 4 failed with parameters: {'xgb_n_estimators': 463, 'xgb_lr': 0.12073522972075314, 'xgb_max_depth': 10, 'xgb_subsample': 0.8724925947108261, 'xgb_colsample': 0.6640726523911299, 'xgb_min_child_weight': 8.09433804411507, 'xgb_gamma': 4.8861900646364065, 'lgbm_n_estimators': 1819, 'lgbm_lr': 0.06401615672984728, 'lgbm_max_depth': 8, 'lgbm_subsample': 0.748015693543082, 'lgbm_colsample': 0.5349196786773558, 'lgbm_num_leaves': 39, 'lgbm_min_child_samples': 38, 'rf_n_estimators': 449, 'rf_max_depth': 10, 'rf_min_samples_split': 12, 'rf_min_samples_leaf': 8} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,912] Trial 4 failed with value None.\n",
      "[W 2025-06-04 20:40:26,926] Trial 11 failed with parameters: {'xgb_n_estimators': 1819, 'xgb_lr': 0.01715136449941957, 'xgb_max_depth': 4, 'xgb_subsample': 0.9645637535407614, 'xgb_colsample': 0.7846506704672155, 'xgb_min_child_weight': 5.460228578380452, 'xgb_gamma': 4.675975931139892, 'lgbm_n_estimators': 2935, 'lgbm_lr': 0.0627464374476297, 'lgbm_max_depth': 3, 'lgbm_subsample': 0.9713046009606479, 'lgbm_colsample': 0.6649621083242061, 'lgbm_num_leaves': 31, 'lgbm_min_child_samples': 27, 'rf_n_estimators': 322, 'rf_max_depth': 3, 'rf_min_samples_split': 9, 'rf_min_samples_leaf': 10} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,928] Trial 11 failed with value None.\n",
      "[W 2025-06-04 20:40:26,962] Trial 6 failed with parameters: {'xgb_n_estimators': 2980, 'xgb_lr': 0.032029625020623516, 'xgb_max_depth': 3, 'xgb_subsample': 0.650194993955224, 'xgb_colsample': 0.9285513097075724, 'xgb_min_child_weight': 3.12358376028195, 'xgb_gamma': 3.5441788469854134, 'lgbm_n_estimators': 511, 'lgbm_lr': 0.08422092685954503, 'lgbm_max_depth': 9, 'lgbm_subsample': 0.8930487466660451, 'lgbm_colsample': 0.7660380215643656, 'lgbm_num_leaves': 140, 'lgbm_min_child_samples': 50, 'rf_n_estimators': 546, 'rf_max_depth': 10, 'rf_min_samples_split': 19, 'rf_min_samples_leaf': 4} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,965] Trial 6 failed with value None.\n",
      "[W 2025-06-04 20:40:26,973] Trial 5 failed with parameters: {'xgb_n_estimators': 594, 'xgb_lr': 0.05280422888768539, 'xgb_max_depth': 8, 'xgb_subsample': 0.5778253251685848, 'xgb_colsample': 0.557711080318038, 'xgb_min_child_weight': 1.7181417007953361, 'xgb_gamma': 2.5519022698133047, 'lgbm_n_estimators': 2167, 'lgbm_lr': 0.00577948192386718, 'lgbm_max_depth': 10, 'lgbm_subsample': 0.7441343570439816, 'lgbm_colsample': 0.9315841709193629, 'lgbm_num_leaves': 95, 'lgbm_min_child_samples': 21, 'rf_n_estimators': 867, 'rf_max_depth': 9, 'rf_min_samples_split': 4, 'rf_min_samples_leaf': 5} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,975] Trial 5 failed with value None.\n",
      "[W 2025-06-04 20:40:26,984] Trial 0 failed with parameters: {'xgb_n_estimators': 1650, 'xgb_lr': 0.013306854199391642, 'xgb_max_depth': 10, 'xgb_subsample': 0.6889582980527447, 'xgb_colsample': 0.5394687051551861, 'xgb_min_child_weight': 5.456542960822197, 'xgb_gamma': 1.8959357080442856, 'lgbm_n_estimators': 2822, 'lgbm_lr': 0.02499351890868018, 'lgbm_max_depth': 3, 'lgbm_subsample': 0.839544473277344, 'lgbm_colsample': 0.8416467534236145, 'lgbm_num_leaves': 99, 'lgbm_min_child_samples': 46, 'rf_n_estimators': 161, 'rf_max_depth': 7, 'rf_min_samples_split': 17, 'rf_min_samples_leaf': 8} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:26,988] Trial 0 failed with value None.\n",
      "[W 2025-06-04 20:40:27,006] Trial 3 failed with parameters: {'xgb_n_estimators': 2506, 'xgb_lr': 0.02914325506618945, 'xgb_max_depth': 10, 'xgb_subsample': 0.8203317705986098, 'xgb_colsample': 0.608789678384668, 'xgb_min_child_weight': 9.637856650664387, 'xgb_gamma': 2.9373117395435937, 'lgbm_n_estimators': 503, 'lgbm_lr': 0.2698782963789782, 'lgbm_max_depth': 12, 'lgbm_subsample': 0.6053800697172766, 'lgbm_colsample': 0.8387939674019269, 'lgbm_num_leaves': 147, 'lgbm_min_child_samples': 29, 'rf_n_estimators': 410, 'rf_max_depth': 4, 'rf_min_samples_split': 5, 'rf_min_samples_leaf': 10} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:27,008] Trial 3 failed with value None.\n",
      "[W 2025-06-04 20:40:27,021] Trial 8 failed with parameters: {'xgb_n_estimators': 1672, 'xgb_lr': 0.04889018692105675, 'xgb_max_depth': 8, 'xgb_subsample': 0.9048662329155928, 'xgb_colsample': 0.5821212679330272, 'xgb_min_child_weight': 9.401551026304759, 'xgb_gamma': 1.1900041684583706, 'lgbm_n_estimators': 668, 'lgbm_lr': 0.1621269256199535, 'lgbm_max_depth': 10, 'lgbm_subsample': 0.7769133089373388, 'lgbm_colsample': 0.8334332633698345, 'lgbm_num_leaves': 71, 'lgbm_min_child_samples': 41, 'rf_n_estimators': 103, 'rf_max_depth': 3, 'rf_min_samples_split': 20, 'rf_min_samples_leaf': 10} because of the following error: ValueError(\"Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 187, in <lambda>\n",
      "    lambda trial: objective(trial, X, y, preprocessor),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aadit\\AppData\\Local\\Temp\\ipykernel_32672\\3343196206.py\", line 150, in objective\n",
      "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 411, in split\n",
      "    for train, test in super().split(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 142, in split\n",
      "    for test_index in self._iter_test_masks(X, y, groups):\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 838, in _iter_test_masks\n",
      "    test_folds = self._make_test_folds(X, y)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py\", line 781, in _make_test_folds\n",
      "    raise ValueError(\n",
      "ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "[W 2025-06-04 20:40:27,023] Trial 8 failed with value None.\n",
      "2025-06-04 20:40:27,024 - ERROR - Error during training: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
      "2025-06-04 20:40:27,025 - ERROR - Error in main execution: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 297\u001b[0m\n\u001b[0;32m    294\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError in main execution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 281\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m X, y, X_test, test_ids \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m    280\u001b[0m preprocessor, numeric_features \u001b[38;5;241m=\u001b[39m get_preprocessor()\n\u001b[1;32m--> 281\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m    283\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(test_preds, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 186\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X, y, preprocessor, numeric_features)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    180\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    181\u001b[0m         storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///optuna_study.db\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    182\u001b[0m         study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar_panel_efficiency\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m         load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     )\n\u001b[1;32m--> 186\u001b[0m     \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7200\u001b[39;49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m    194\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:100\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[38;5;66;03m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n\u001b[1;32m--> 100\u001b[0m                         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m                 futures\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    103\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    104\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m                     )\n\u001b[0;32m    116\u001b[0m                 )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 187\u001b[0m, in \u001b[0;36mtrain_and_evaluate.<locals>.<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    180\u001b[0m         direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    181\u001b[0m         storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///optuna_study.db\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    182\u001b[0m         study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolar_panel_efficiency\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    183\u001b[0m         load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     )\n\u001b[0;32m    186\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m--> 187\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    188\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    189\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    190\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7200\u001b[39m\n\u001b[0;32m    191\u001b[0m     )\n\u001b[0;32m    193\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[0;32m    194\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 150\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, X, y, preprocessor)\u001b[0m\n\u001b[0;32m    147\u001b[0m kf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m    148\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 150\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:411\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    405\u001b[0m         (\n\u001b[0;32m    406\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    409\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:142\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    140\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    141\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m--> 142\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_test_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:838\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 838\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[1;32mc:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:781\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    779\u001b[0m allowed_target_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target_y \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_target_types:\n\u001b[1;32m--> 781\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    782\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported target types are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    783\u001b[0m             allowed_target_types, type_of_target_y\n\u001b[0;32m    784\u001b[0m         )\n\u001b[0;32m    785\u001b[0m     )\n\u001b[0;32m    787\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y)\n\u001b[0;32m    789\u001b[0m _, y_idx, y_inv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead."
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback, LightGBMPruningCallback\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 1. Data Loading with Robust Type Conversion and Outlier Handling\n",
    "def load_data():\n",
    "    \"\"\"Load and prepare data with proper type conversion, feature engineering, and outlier handling\"\"\"\n",
    "    try:\n",
    "        train_data = pd.read_csv('dataset/train.csv')\n",
    "        test_data = pd.read_csv('dataset/test.csv')\n",
    "        logging.info(\"Data loaded successfully\")\n",
    "        \n",
    "        numeric_cols = ['temperature', 'irradiance', 'humidity', 'panel_age', \n",
    "                        'maintenance_count', 'soiling_ratio', 'voltage', 'current',\n",
    "                        'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure']\n",
    "        categorical_cols = ['string_id', 'error_code', 'installation_type']\n",
    "        \n",
    "        # Convert numeric columns and handle outliers\n",
    "        for col in numeric_cols:\n",
    "            train_data[col] = pd.to_numeric(train_data[col], errors='coerce')\n",
    "            test_data[col] = pd.to_numeric(test_data[col], errors='coerce')\n",
    "            train_data[col] = train_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            test_data[col] = test_data[col].replace([np.inf, -np.inf], np.nan)\n",
    "            train_data[col] = np.where(train_data[col] < 0, np.nan, train_data[col])\n",
    "            test_data[col] = np.where(test_data[col] < 0, np.nan, test_data[col])\n",
    "            # Clip outliers at 1st and 99th percentiles\n",
    "            p1, p99 = train_data[col].quantile([0.01, 0.99]).values\n",
    "            train_data[col] = train_data[col].clip(p1, p99)\n",
    "            test_data[col] = test_data[col].clip(p1, p99)\n",
    "        \n",
    "        # Advanced feature engineering\n",
    "        for data in [train_data, test_data]:\n",
    "            data['power_output'] = data['voltage'] * data['current']\n",
    "            data['temp_adjusted_irradiance'] = data['irradiance'] / (1 + 0.005 * (data['module_temperature'].fillna(25) - 25))\n",
    "            data['soiling_impact'] = data['soiling_ratio'] * data['irradiance']\n",
    "            data['age_maintenance_ratio'] = data['panel_age'] / (data['maintenance_count'].fillna(0) + 1)\n",
    "            data['log_irradiance'] = np.log1p(data['irradiance'].fillna(0))\n",
    "            data['temp_diff'] = data['module_temperature'] - data['temperature']\n",
    "            data['irradiance_per_age'] = data['irradiance'] / (data['panel_age'].fillna(1) + 1)\n",
    "            data['wind_cooling'] = data['wind_speed'] * (data['module_temperature'].fillna(25) - data['temperature'].fillna(25))\n",
    "            data['humidity_pressure_ratio'] = data['humidity'] / (data['pressure'].fillna(1000) + 1)\n",
    "            data['cloud_irradiance_interaction'] = data['cloud_coverage'] * data['irradiance']\n",
    "        \n",
    "        logging.info(\"Feature engineering completed\")\n",
    "        logging.info(f\"NaN counts in train_data: {train_data[numeric_cols + ['power_output', 'temp_adjusted_irradiance', 'soiling_impact', 'age_maintenance_ratio', 'log_irradiance', 'temp_diff', 'irradiance_per_age', 'wind_cooling', 'humidity_pressure_ratio', 'cloud_irradiance_interaction']].isna().sum()}\")\n",
    "        \n",
    "        X = train_data.drop(columns=['id', 'efficiency'])\n",
    "        y = train_data['efficiency']\n",
    "        test_ids = test_data['id']\n",
    "        X_test = test_data.drop(columns=['id'])\n",
    "        \n",
    "        # Verify target variable\n",
    "        logging.info(f\"Target variable dtype: {y.dtype}, NaN count: {y.isna().sum()}\")\n",
    "        if not np.issubdtype(y.dtype, np.number):\n",
    "            raise ValueError(\"Target variable 'efficiency' must be numeric\")\n",
    "        \n",
    "        return X, y, X_test, test_ids\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 2. Enhanced Preprocessing with OrdinalEncoder\n",
    "def get_preprocessor():\n",
    "    \"\"\"Create robust preprocessing pipeline with advanced imputation and encoding\"\"\"\n",
    "    numeric_features = ['temperature', 'irradiance', 'humidity', 'panel_age', \n",
    "                        'maintenance_count', 'soiling_ratio', 'voltage', 'current',\n",
    "                        'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure',\n",
    "                        'power_output', 'temp_adjusted_irradiance', 'soiling_impact',\n",
    "                        'age_maintenance_ratio', 'log_irradiance', 'temp_diff',\n",
    "                        'irradiance_per_age', 'wind_cooling', 'humidity_pressure_ratio',\n",
    "                        'cloud_irradiance_interaction']\n",
    "    \n",
    "    categorical_features = ['string_id', 'error_code', 'installation_type']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('scaler', RobustScaler())])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ordinal_enc', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)],\n",
    "        remainder='drop')\n",
    "    \n",
    "    return preprocessor, numeric_features\n",
    "\n",
    "# 3. Hyperparameter Tuning with Optuna\n",
    "def objective(trial, X, y, preprocessor):\n",
    "    \"\"\"Objective function for Optuna hyperparameter optimization with pruning\"\"\"\n",
    "    params_xgb = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 3000),\n",
    "        'learning_rate': trial.suggest_float('xgb_lr', 0.005, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample', 0.5, 1.0),\n",
    "        'min_child_weight': trial.suggest_float('xgb_min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('xgb_gamma', 0, 5)\n",
    "    }\n",
    "    params_lgbm = {\n",
    "        'n_estimators': trial.suggest_int('lgbm_n_estimators', 100, 3000),\n",
    "        'learning_rate': trial.suggest_float('lgbm_lr', 0.005, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('lgbm_max_depth', 3, 12),\n",
    "        'subsample': trial.suggest_float('lgbm_subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('lgbm_colsample', 0.5, 1.0),\n",
    "        'num_leaves': trial.suggest_int('lgbm_num_leaves', 20, 150),\n",
    "        'min_child_samples': trial.suggest_int('lgbm_min_child_samples', 10, 50)\n",
    "    }\n",
    "    params_rf = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 3, 12),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(**params_xgb, random_state=42, early_stopping_rounds=50, eval_metric='rmse')\n",
    "    lgbm = LGBMRegressor(**params_lgbm, random_state=42, early_stopping_rounds=50)\n",
    "    rf = RandomForestRegressor(**params_rf, random_state=42)\n",
    "    \n",
    "    xgb_pruner = XGBoostPruningCallback(trial, 'validation_0-rmse')\n",
    "    lgbm_pruner = LightGBMPruningCallback(trial, 'rmse')\n",
    "    \n",
    "    ensemble = VotingRegressor([('xgb', xgb), ('lgbm', lgbm), ('rf', rf)])\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', ensemble)\n",
    "        ])\n",
    "        \n",
    "        pipeline.fit(\n",
    "            X_train, y_train,\n",
    "            regressor__xgb__eval_set=[(X_val, y_val)],\n",
    "            regressor__lgbm__eval_set=[(X_val, y_val)],\n",
    "            regressor__xgb__callbacks=[xgb_pruner],\n",
    "            regressor__lgbm__callbacks=[lgbm_pruner],\n",
    "            regressor__xgb__verbose=False,\n",
    "            regressor__lgbm__verbose=-1\n",
    "        )\n",
    "        \n",
    "        val_preds = pipeline.predict(X_val)\n",
    "        score = 100 * (1 - np.sqrt(mean_squared_error(y_val, val_preds)))\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# 4. Model Training with Cross-Validation, Feature Selection, and Ensemble\n",
    "def train_and_evaluate(X, y, preprocessor, numeric_features):\n",
    "    \"\"\"Train ensemble model with cross-validation and feature selection\"\"\"\n",
    "    try:\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            storage='sqlite:///optuna_study.db',\n",
    "            study_name='solar_panel_efficiency',\n",
    "            load_if_exists=True\n",
    "        )\n",
    "        \n",
    "        study.optimize(\n",
    "            lambda trial: objective(trial, X, y, preprocessor),\n",
    "            n_trials=30,\n",
    "            n_jobs=-1,\n",
    "            timeout=7200\n",
    "        )\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        logging.info(f\"Best hyperparameters: {best_params}\")\n",
    "        logging.info(f\"Best CV score: {study.best_value:.2f}\")\n",
    "        \n",
    "        xgb = XGBRegressor(\n",
    "            n_estimators=best_params['xgb_n_estimators'],\n",
    "            learning_rate=best_params['xgb_lr'],\n",
    "            max_depth=best_params['xgb_max_depth'],\n",
    "            subsample=best_params['xgb_subsample'],\n",
    "            colsample_bytree=best_params['xgb_colsample'],\n",
    "            min_child_weight=best_params['xgb_min_child_weight'],\n",
    "            gamma=best_params['xgb_gamma'],\n",
    "            random_state=42,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        lgbm = LGBMRegressor(\n",
    "            n_estimators=best_params['lgbm_n_estimators'],\n",
    "            learning_rate=best_params['lgbm_lr'],\n",
    "            max_depth=best_params['lgbm_max_depth'],\n",
    "            subsample=best_params['lgbm_subsample'],\n",
    "            colsample_bytree=best_params['lgbm_colsample'],\n",
    "            num_leaves=best_params['lgbm_num_leaves'],\n",
    "            min_child_samples=best_params['lgbm_min_child_samples'],\n",
    "            random_state=42,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=best_params['rf_n_estimators'],\n",
    "            max_depth=best_params['rf_max_depth'],\n",
    "            min_samples_split=best_params['rf_min_samples_split'],\n",
    "            min_samples_leaf=best_params['rf_min_samples_leaf'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        ensemble = VotingRegressor([('xgb', xgb), ('lgbm', lgbm), ('rf', rf)])\n",
    "        \n",
    "        full_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('selector', SelectFromModel(XGBRegressor(random_state=42), max_features=15)),\n",
    "            ('regressor', ensemble)\n",
    "        ])\n",
    "        \n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores, mae_scores, r2_scores = [], [], []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X, pd.qcut(y, q=5, duplicates='drop')):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            full_pipeline.fit(\n",
    "                X_train, y_train,\n",
    "                regressor__xgb__eval_set=[(X_val, y_val)],\n",
    "                regressor__lgbm__eval_set=[(X_val, y_val)],\n",
    "                regressor__xgb__verbose=False,\n",
    "                regressor__lgbm__verbose=-1\n",
    "            )\n",
    "            \n",
    "            val_preds = full_pipeline.predict(X_val)\n",
    "            score = 100 * (1 - np.sqrt(mean_squared_error(y_val, val_preds)))\n",
    "            mae = mean_absolute_error(y_val, val_preds)\n",
    "            r2 = r2_score(y_val, val_preds)\n",
    "            scores.append(score)\n",
    "            mae_scores.append(mae)\n",
    "            r2_scores.append(r2)\n",
    "        \n",
    "        logging.info(f\"Cross-Validation Scores: {scores}\")\n",
    "        logging.info(f\"Mean CV Score: {np.mean(scores):.2f}  {np.std(scores):.2f}\")\n",
    "        logging.info(f\"Mean MAE: {np.mean(mae_scores):.4f}  {np.std(mae_scores):.4f}\")\n",
    "        logging.info(f\"Mean R: {np.mean(r2_scores):.4f}  {np.std(r2_scores):.4f}\")\n",
    "        \n",
    "        full_pipeline.fit(X, y)\n",
    "        \n",
    "        feature_importance = full_pipeline.named_steps['selector'].estimator_.feature_importances_\n",
    "        feature_names = numeric_features + categorical_features\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "        logging.info(f\"Feature Importance:\\n{importance_df.sort_values(by='Importance', ascending=False)}\")\n",
    "        \n",
    "        return full_pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 5. Main Execution Flow\n",
    "def main():\n",
    "    try:\n",
    "        X, y, X_test, test_ids = load_data()\n",
    "        preprocessor, numeric_features = get_preprocessor()\n",
    "        model = train_and_evaluate(X, y, preprocessor, numeric_features)\n",
    "        test_preds = model.predict(X_test)\n",
    "        test_preds = np.clip(test_preds, 0, 1)\n",
    "        \n",
    "        submission = pd.DataFrame({'id': test_ids, 'efficiency': test_preds})\n",
    "        if submission.shape == (12000, 2) and list(submission.columns) == ['id', 'efficiency']:\n",
    "            submission.to_csv('submission.csv', index=False)\n",
    "            logging.info(\"Submission file created successfully!\")\n",
    "        else:\n",
    "            logging.error(\"Invalid submission format\")\n",
    "            raise ValueError(\"Submission file does not meet required format (12000 x 2 with columns 'id', 'efficiency')\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
